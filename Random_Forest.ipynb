{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs9mbUGVBakaz6PJbXmIRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbonelli/Cientista-de-Dados_EBAC/blob/main/Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üå≥ Random Forest"
      ],
      "metadata": {
        "id": "-w76JuWKgUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. O que √© Random Forest?\n",
        "\n",
        "O **Random Forest** √© um algoritmo de aprendizado de m√°quina que constr√≥i v√°rias √°rvores de decis√£o diferentes e combina os resultados delas para fazer uma previs√£o final.\n",
        "\n",
        "A diferen√ßa principal em rela√ß√£o ao Bagging √© que o Random Forest **al√©m de fazer amostragem com reposi√ß√£o (bootstrap)**, tamb√©m **escolhe aleatoriamente apenas algumas features (colunas)** para cada √°rvore. Isso diminui a correla√ß√£o entre elas e aumenta a robustez do modelo.\n",
        "\n",
        "Como resultado, temos um modelo mais **preciso, est√°vel e resistente ao overfitting** do que uma √∫nica √°rvore de decis√£o.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JRHLPTmkgijm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úÖ 2. Passo a passo para o algoritmo Random Forest\n",
        "\n",
        "\n",
        "1. **Bootstrap**\n",
        "   - Criar v√°rias amostras com reposi√ß√£o a partir do conjunto de dados original.\n",
        "\n",
        "2. **Sele√ß√£o aleat√≥ria de features**\n",
        "   - Para cada √°rvore, escolher aleatoriamente um subconjunto de colunas (features) para treinar, o que gera √°rvores diferentes e menos correlacionadas.\n",
        "\n",
        "3. **Modelagem com Decision Trees**\n",
        "   - Treinar uma √°rvore de decis√£o em cada amostra com as features selecionadas.\n",
        "\n",
        "4. **Agrega√ß√£o**\n",
        "   - Combinar as previs√µes das √°rvores:\n",
        "     - Para classifica√ß√£o: usar vota√ß√£o (classe mais votada).\n",
        "     - Para regress√£o: usar a m√©dia das previs√µes.\n",
        "\n",
        "   ---"
      ],
      "metadata": {
        "id": "ldS7zCnXgswv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. Qual a diferen√ßa entre Bagging e Random Forest?"
      ],
      "metadata": {
        "id": "BDZyI48Bod1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Caracter√≠stica              | **Bagging**                              | **Random Forest**                                 |\n",
        "|----------------------------|------------------------------------------|---------------------------------------------------|\n",
        "| Tipo de modelo base        | Qualquer (mas geralmente Decision Trees) | Apenas √Årvores de Decis√£o                        |\n",
        "| Bootstrap (amostragem)     | ‚úÖ Sim                                   | ‚úÖ Sim                                            |\n",
        "| Sele√ß√£o aleat√≥ria de features | ‚ùå N√£o (usa todas)                      | ‚úÖ Sim (subset aleat√≥rio por √°rvore)             |\n",
        "| Correla√ß√£o entre √°rvores   | Alta                                     | Baixa (por causa da sele√ß√£o de features)         |\n",
        "| Vota√ß√£o/M√©dia              | ‚úÖ Sim                                   | ‚úÖ Sim                                            |\n",
        "| Exemplo em sklearn         | `BaggingClassifier`                      | `RandomForestClassifier`                         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6OmuxDPoohnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 4. Implementa√ß√£o em Python\n",
        "\n",
        "Vamos usar o conjunto de dados Iris como exemplo e aplicar o Randon Farest, sendo demonstrando passo a passo de forma manual e depois usando scikit-learn\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aroFvdA8g551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üå≥ 4.1. Random Forest com Dataset Iris - Manual\n",
        "\n",
        "Vamos entender o funcionamento do Randon forest recriando suas 4 etapas principais com o dataset Iris.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9ZeGybOtio5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 1: Bootstrap (amostragem com reposi√ß√£o)"
      ],
      "metadata": {
        "id": "Cpz7qHvhkxAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar dataset Iris\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Dividir dados para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "k339usHWi5LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Etapa 2: Feature Selection (sele√ß√£o aleat√≥ria de colunas)"
      ],
      "metadata": {
        "id": "H8LWfeWKpyHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para selecionar aleatoriamente k features\n",
        "def selecionar_features(X, k, random_state=None):\n",
        "    np.random.seed(random_state)\n",
        "    cols = np.random.choice(X.columns, size=k, replace=False)\n",
        "    return X[cols], cols"
      ],
      "metadata": {
        "id": "CUsMMmNup6kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå± Etapa 3: Treinar v√°rias √Årvores com subsets diferentes"
      ],
      "metadata": {
        "id": "JUKjM5Mwi8wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "n_arvores = 10\n",
        "k_features = 2\n",
        "modelos = []\n",
        "colunas_usadas = []\n",
        "\n",
        "for i in range(n_arvores):\n",
        "    # Etapa 1: Bootstrap\n",
        "    X_sample, y_sample = resample(X_train, y_train, replace=True, random_state=i)\n",
        "\n",
        "    # Etapa 2: Feature Selection\n",
        "    X_sub, cols = selecionar_features(X_sample, k=k_features, random_state=i)\n",
        "\n",
        "    # Etapa 3: Modelagem\n",
        "    modelo = DecisionTreeClassifier(random_state=i)\n",
        "    modelo.fit(X_sub, y_sample)\n",
        "\n",
        "    modelos.append(modelo)\n",
        "    colunas_usadas.append(cols)"
      ],
      "metadata": {
        "id": "QaqpqeD_i_Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Etapa 4: Agrega√ß√£o das Previs√µes (Vota√ß√£o)\n"
      ],
      "metadata": {
        "id": "nvibPlZGjNY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fazer previs√µes com as √°rvores\n",
        "previsoes = []\n",
        "\n",
        "for modelo, cols in zip(modelos, colunas_usadas):\n",
        "    X_test_sub = X_test[cols]\n",
        "    pred = modelo.predict(X_test_sub)\n",
        "    previsoes.append(pred)\n",
        "\n",
        "# Vota√ß√£o majorit√°ria\n",
        "previsoes = np.array(previsoes)\n",
        "final = mode(previsoes, axis=0, keepdims=True).mode[0]\n",
        "\n",
        "print(\"‚úÖ Acur√°cia do Random Forest manual:\", accuracy_score(y_test, final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6-MVLYwjPAB",
        "outputId": "19407b84-984d-4675-d93a-456ceb04698e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Acur√°cia do Random Forest manual: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° 4.2. Random Forest com Scikit-learn (autom√°tico)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlu-mwBNj90I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar dados\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Criar Random Forest com 10 √°rvores\n",
        "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Previs√£o e avalia√ß√£o\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"‚úÖ Acur√°cia com RandomForestClassifier:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsdlCeFkT62",
        "outputId": "89b82142-e47d-407c-8b8b-c723725ce827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Acur√°cia com RandomForestClassifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_duPwIglsicu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üå≤ Hiperpar√¢metros do Random Forest\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5oVk7bJ2skE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. Quais s√£o os principais hiperpar√¢metros do Random Forest?\n",
        "\n",
        "- `n_estimators`: n√∫mero de √°rvores na floresta.\n",
        "- `max_depth`: profundidade m√°xima de cada √°rvore.\n",
        "- `min_samples_split`: n√∫mero m√≠nimo de amostras para dividir um n√≥.\n",
        "- `min_samples_leaf`: n√∫mero m√≠nimo de amostras em uma folha.\n",
        "- `max_features`: n√∫mero de features consideradas em cada divis√£o.\n",
        "- `bootstrap`: se usa amostragem com reposi√ß√£o (bootstrap).\n",
        "- `random_state`: semente para reprodutibilidade.\n",
        "- `criterion`: fun√ß√£o para medir a qualidade da divis√£o (`gini`, `entropy`).\n",
        "- `oob_score`: se calcula a acur√°cia usando os dados fora da amostra (out-of-bag).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 2. Para que serve cada um deles?\n",
        "\n",
        "| Hiperpar√¢metro        | Fun√ß√£o                                                                 |\n",
        "|-----------------------|------------------------------------------------------------------------|\n",
        "| `n_estimators`        | Quantidade de √°rvores. Mais √°rvores = mais robustez (at√© certo ponto). |\n",
        "| `max_depth`           | Limita a profundidade da √°rvore, evitando overfitting.                |\n",
        "| `min_samples_split`   | M√≠nimo de amostras para dividir um n√≥. Evita divis√µes com poucos dados. |\n",
        "| `min_samples_leaf`    | M√≠nimo de amostras em uma folha. Ajuda na regulariza√ß√£o.              |\n",
        "| `max_features`        | N√∫mero de features a considerar em cada divis√£o. Reduz correla√ß√£o entre √°rvores. |\n",
        "| `bootstrap`           | Se `True`, usa amostragem com reposi√ß√£o.                              |\n",
        "| `random_state`        | Garante que os resultados sejam reproduz√≠veis.                        |\n",
        "| `criterion`           | Crit√©rio de divis√£o: `gini` ou `entropy`.                             |\n",
        "| `oob_score`           | Avalia performance usando dados fora da amostra (sem usar `test`).    |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Hiperpar√¢metros mais ajustados por profissionais\n",
        "\n",
        "Os profissionais geralmente ajustam:\n",
        "\n",
        "- `n_estimators`: 100 a 500\n",
        "- `max_depth`: 5 a 20 (ou `None`)\n",
        "- `min_samples_split`: 2, 5 ou 10\n",
        "- `min_samples_leaf`: 1, 2 ou 4\n",
        "- `max_features`: `'sqrt'` (mais comum), `'log2'`, ou valor num√©rico\n",
        "- `bootstrap`: `True` na maioria dos casos\n",
        "\n",
        "Exemplo de configura√ß√£o comum:\n",
        "```python\n",
        "RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "NfE_3Tehub6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Exemplo com GridSearchCV para ajuste de hiperpar√¢metros"
      ],
      "metadata": {
        "id": "-SVj5P0auzXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar dados\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Definir modelo e par√¢metros\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "parametros = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Rodar GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=parametros, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Avaliar\n",
        "print(\"Melhores par√¢metros:\", grid_search.best_params_)\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW5HrDxLu4Tm",
        "outputId": "1bd6ba00-a10c-4e35-e2f9-8658245adfb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
            "Melhores par√¢metros: {'bootstrap': True, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Acur√°cia: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "1080 fits failed out of a total of 3240.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "427 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "653 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.95494071 0.9458498  0.95494071\n",
            " 0.95494071 0.95454545 0.95494071 0.95494071 0.96363636 0.95494071\n",
            " 0.95454545 0.95454545 0.9458498  0.95454545 0.95454545 0.9458498\n",
            " 0.95454545 0.95454545 0.9458498  0.94624506 0.95454545 0.9458498\n",
            " 0.94624506 0.95454545 0.9458498  0.95494071 0.95454545 0.9458498\n",
            " 0.95494071 0.9458498  0.95494071 0.95494071 0.95454545 0.95494071\n",
            " 0.95494071 0.96363636 0.95494071 0.95454545 0.95454545 0.9458498\n",
            " 0.95454545 0.95454545 0.9458498  0.95454545 0.95454545 0.9458498\n",
            " 0.94624506 0.95454545 0.9458498  0.94624506 0.95454545 0.9458498\n",
            " 0.95494071 0.95454545 0.9458498         nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.9458498  0.9458498  0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.95494071 0.95454545 0.95494071 0.9458498  0.94545455 0.9458498\n",
            " 0.93675889 0.95454545 0.9458498  0.9458498  0.95454545 0.95454545\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.9458498  0.9458498  0.9458498  0.95494071 0.95454545 0.95494071\n",
            " 0.9458498  0.94545455 0.9458498  0.93675889 0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.95454545 0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.93715415 0.9458498  0.9458498\n",
            " 0.9458498  0.9458498  0.9458498  0.95494071 0.9458498  0.95494071\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.95454545\n",
            " 0.9458498  0.95454545 0.95454545 0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            " 0.93715415 0.9458498  0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.95494071 0.9458498  0.95494071 0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.95454545 0.9458498  0.95454545 0.95454545\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498         nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.93715415 0.9458498  0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.95494071 0.9458498  0.95494071 0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.95454545 0.9458498  0.95454545 0.95454545\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498  0.93715415 0.9458498  0.9458498\n",
            " 0.9458498  0.9458498  0.9458498  0.95494071 0.9458498  0.95494071\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.95454545\n",
            " 0.9458498  0.95454545 0.95454545 0.9458498  0.95454545 0.9458498\n",
            " 0.9458498  0.95454545 0.9458498  0.9458498  0.95454545 0.9458498\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.94624506 0.94624506 0.93715415\n",
            " 0.94624506 0.94624506 0.93715415 0.94624506 0.94624506 0.94624506\n",
            " 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415\n",
            " 0.94624506 0.94624506 0.93715415 0.93715415 0.93715415 0.93715415\n",
            " 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415\n",
            " 0.94624506 0.94624506 0.93715415 0.94624506 0.94624506 0.93715415\n",
            " 0.94624506 0.94624506 0.94624506 0.93715415 0.93715415 0.93715415\n",
            " 0.93715415 0.93715415 0.93715415 0.94624506 0.94624506 0.93715415\n",
            " 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415 0.93715415\n",
            " 0.93715415 0.93715415 0.93715415        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.9458498  0.9458498  0.9458498  0.93675889 0.93675889 0.93675889\n",
            " 0.95494071 0.9458498  0.9458498  0.93675889 0.93675889 0.93675889\n",
            " 0.93675889 0.93675889 0.9458498  0.9458498  0.95494071 0.95494071\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.93675889 0.93675889 0.93675889 0.95494071 0.9458498  0.9458498\n",
            " 0.93675889 0.93675889 0.93675889 0.93675889 0.93675889 0.9458498\n",
            " 0.9458498  0.95494071 0.95494071 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.9458498\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.9458498  0.9458498  0.9458498\n",
            " 0.93675889 0.93675889 0.93675889 0.95494071 0.9458498  0.9458498\n",
            " 0.93675889 0.9458498  0.93675889 0.93675889 0.9458498  0.9458498\n",
            " 0.9458498  0.95494071 0.95494071 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.9458498\n",
            " 0.9458498  0.9458498  0.9458498  0.93675889 0.93675889 0.93675889\n",
            " 0.95494071 0.9458498  0.9458498  0.93675889 0.9458498  0.93675889\n",
            " 0.93675889 0.9458498  0.9458498  0.9458498  0.95494071 0.95494071\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.9458498         nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.9458498  0.9458498  0.9458498  0.93675889 0.93675889 0.93675889\n",
            " 0.95494071 0.9458498  0.9458498  0.93675889 0.9458498  0.93675889\n",
            " 0.93675889 0.9458498  0.9458498  0.9458498  0.95494071 0.95494071\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.9458498  0.9458498  0.9458498  0.9458498\n",
            " 0.93675889 0.93675889 0.93675889 0.95494071 0.9458498  0.9458498\n",
            " 0.93675889 0.9458498  0.93675889 0.93675889 0.9458498  0.9458498\n",
            " 0.9458498  0.95494071 0.95494071 0.9458498  0.9458498  0.93715415\n",
            " 0.9458498  0.9458498  0.93715415 0.9458498  0.9458498  0.9458498 ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}