{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNnINZWmA5DF4XNN0GFR5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbonelli/Cientista-de-Dados_EBAC/blob/main/GBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Gradient Boosting Machine (GBM)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-w76JuWKgUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. Cinco diferen√ßas entre AdaBoost e GBM\n",
        "\n",
        "| Caracter√≠stica             | AdaBoost                                             | GBM (Gradient Boosting Machine)                          |\n",
        "|----------------------------|------------------------------------------------------|-----------------------------------------------------------|\n",
        "| Atualiza√ß√£o dos pesos     | Ajusta pesos dos exemplos com base nos erros        | Minimiza fun√ß√£o de perda diretamente                      |\n",
        "| Tipo de fun√ß√£o de perda   | Usa erro classificat√≥rio                            | Pode usar MSE, MAE, LogLoss, etc. (mais flex√≠vel)         |\n",
        "| Algoritmo de aprendizado  | Sequencial adaptativo                               | Otimiza√ß√£o via gradiente da fun√ß√£o de perda               |\n",
        "| Sensibilidade a outliers  | Alta (por causa dos pesos)                          | Pode ser ajustada com `loss` e `learning_rate`            |\n",
        "| Nome alternativo          | Adaptive Boosting                                   | Gradient Boosting                                         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xwTdFzktiuos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 2. Exemplos de Classifica√ß√£o e Regress√£o com GBM (`GradientBoostingClassifier` e `Regressor`)\n"
      ],
      "metadata": {
        "id": "aroFvdA8g551"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.datasets import load_iris, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# CLASSIFICA√á√ÉO\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"‚úÖ Acur√°cia (Classifica√ß√£o):\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# REGRESS√ÉO\n",
        "Xr, yr = make_regression(n_samples=200, n_features=4, noise=0.1, random_state=42)\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, random_state=42)\n",
        "\n",
        "reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "reg.fit(Xr_train, yr_train)\n",
        "yr_pred = reg.predict(Xr_test)\n",
        "print(\"‚úÖ Erro Quadr√°tico M√©dio (Regress√£o):\", mean_squared_error(yr_test, yr_pred))"
      ],
      "metadata": {
        "id": "ynLLi7V8XGe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65af0b23-a153-4515-a209-8f872fb5f97f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Acur√°cia (Classifica√ß√£o): 1.0\n",
            "‚úÖ Erro Quadr√°tico M√©dio (Regress√£o): 951.4441426212903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. Cinco hiperpar√¢metros importantes no Gradient Boosting (GBM)\n",
        "\n",
        "| Hiperpar√¢metro     | Descri√ß√£o                                                                                   |\n",
        "|--------------------|----------------------------------------------------------------------------------------------|\n",
        "| `n_estimators`     | N√∫mero total de √°rvores (modelos fracos) treinadas sequencialmente.                         |\n",
        "| `learning_rate`    | Controla o impacto de cada √°rvore na predi√ß√£o final. Valores menores exigem mais √°rvores.  |\n",
        "| `max_depth`        | Profundidade m√°xima de cada √°rvore. Limita a complexidade e ajuda a evitar overfitting.     |\n",
        "| `subsample`        | Propor√ß√£o dos dados de treino usados em cada √°rvore (Stochastic GBM se < 1.0).              |\n",
        "| `loss`             | Fun√ß√£o de perda a ser minimizada (ex: `log_loss` para classifica√ß√£o, `squared_error` para regress√£o). |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Observa√ß√µes:\n",
        "\n",
        "- **`n_estimators`** e **`learning_rate`** t√™m uma rela√ß√£o inversa: quanto menor o learning rate, mais √°rvores geralmente s√£o necess√°rias.\n",
        "- **`max_depth`** controla a complexidade de cada √°rvore ‚Äî profundidades maiores capturam padr√µes mais complexos, mas podem causar overfitting.\n",
        "- **`subsample < 1.0`** ativa o modo *Stochastic GBM*, introduzindo aleatoriedade e melhorando a capacidade de generaliza√ß√£o.\n",
        "- **`loss`** depende da tarefa: `log_loss` √© padr√£o para classifica√ß√£o, `squared_error` para regress√£o.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Dica pr√°tica:\n",
        "\n",
        "Comece com:\n",
        "\n",
        "```python\n",
        "GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    loss='log_loss',\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "wsYgVO5RXNXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 4. Ajuste autom√°tico com GridSearchCV"
      ],
      "metadata": {
        "id": "-SVj5P0auzXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [2, 3, 5],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"üéØ Melhores par√¢metros:\", grid.best_params_)\n",
        "print(\"‚úÖ Acur√°cia com modelo ajustado:\", accuracy_score(y_test, grid.best_estimator_.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmOGUo66jhMs",
        "outputId": "3c841673-cc25-4ddc-be08-9c0a0b6faa46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
            "üéØ Melhores par√¢metros: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.6}\n",
            "‚úÖ Acur√°cia com modelo ajustado: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 5. Diferen√ßa entre GBM tradicional e Stochastic GBM\n",
        "\n",
        "üìñ Segundo o artigo **\"Stochastic Gradient Boosting\"** de Jerome Friedman (1999):\n",
        "\n",
        "### üéØ A maior diferen√ßa entre os dois algoritmos est√° no uso de **subamostragem aleat√≥ria** dos dados.\n",
        "\n",
        "| Caracter√≠stica             | GBM Tradicional                              | Stochastic GBM                                  |\n",
        "|----------------------------|-----------------------------------------------|-------------------------------------------------|\n",
        "| Dados usados por itera√ß√£o  | 100% dos dados de treino                      | Apenas uma fra√ß√£o dos dados (ex: 50% ou 80%)    |\n",
        "| Par√¢metro chave            | ‚Äî                                             | `subsample < 1.0` no `GradientBoosting`         |\n",
        "| Varia√ß√£o/aleatoriedade     | Nenhuma (determin√≠stico)                      | Introduzida via amostragem sem reposi√ß√£o        |\n",
        "| Vantagens                  | Aprendizado mais est√°vel                     | Reduz overfitting e melhora generaliza√ß√£o       |\n",
        "| Nome dado por Friedman     | ‚Äî                                             | **Stochastic Gradient Boosting**                |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Em resumo:\n",
        "\n",
        "> **A principal diferen√ßa entre o GBM tradicional e o Stochastic GBM √© que o Stochastic utiliza apenas uma amostra aleat√≥ria dos dados de treino em cada itera√ß√£o (definida pelo par√¢metro `subsample`), enquanto o GBM tradicional usa todos os dados.**\n",
        "\n",
        "Essa estrat√©gia de aleatoriedade controlada **ajuda a reduzir o overfitting** e torna o modelo mais **robusto em dados ruidosos ou com muitos exemplos similares**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "os7D-zfKjxLc"
      }
    }
  ]
}