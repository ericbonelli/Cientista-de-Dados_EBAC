{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMydrN6OXui2PnnCbPn1ZEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbonelli/Cientista-de-Dados_EBAC/blob/main/AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö°  AdaBoost"
      ],
      "metadata": {
        "id": "-w76JuWKgUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. O que √© o AdaBoost?\n",
        "\n",
        "**AdaBoost** (Adaptive Boosting) √© um algoritmo de aprendizado de m√°quina que combina v√°rios modelos fracos (geralmente √°rvores simples, chamadas \"stumps\") para formar um modelo forte e mais preciso.\n",
        "\n",
        "A ideia central √© **focar nos exemplos que o modelo anterior errou**, ajustando os pesos dos dados a cada rodada de treinamento. Isso permite que o modelo aprenda progressivamente com seus pr√≥prios erros.\n",
        "\n",
        "Ele funciona bem com classificadores fracos e √© muito usado por sua simplicidade e boa performance ‚Äî desde que os dados n√£o tenham muito **ru√≠do ou outliers**, j√° que o AdaBoost √© sens√≠vel a isso.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JRHLPTmkgijm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 2. Passo a passo do algoritmo AdaBoost\n",
        "\n",
        "1. **Inicializa√ß√£o dos pesos**  \n",
        "   - Todos os exemplos do dataset come√ßam com o mesmo peso.\n",
        "\n",
        "2. **Treinamento sequencial de modelos fracos**  \n",
        "   - Treina-se um modelo (ex: √°rvore simples).\n",
        "   - Calcula-se o erro ponderado (considerando os pesos dos exemplos).\n",
        "\n",
        "3. **Ajuste dos pesos dos exemplos**  \n",
        "   - Exemplos que o modelo errou recebem mais peso (para ganhar mais aten√ß√£o no pr√≥ximo modelo).\n",
        "\n",
        "4. **C√°lculo do peso do modelo**  \n",
        "   - Modelos com menor erro ganham mais peso na combina√ß√£o final.\n",
        "\n",
        "5. **Repeti√ß√£o**  \n",
        "   - Repete-se o processo para `n_estimators` vezes.\n",
        "\n",
        "6. **Agrega√ß√£o das previs√µes**  \n",
        "   - A previs√£o final √© feita com base em uma vota√ß√£o ponderada dos modelos.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ldS7zCnXgswv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. Cinco diferen√ßas entre Random Forest e AdaBoost\n",
        "\n",
        "| Caracter√≠stica           | **Random Forest**                              | **AdaBoost**                                      |\n",
        "|--------------------------|-------------------------------------------------|--------------------------------------------------|\n",
        "| Estrat√©gia de ensemble   | Paralela (v√°rias √°rvores ao mesmo tempo)       | Sequencial (um modelo por vez)                   |\n",
        "| Peso dos modelos         | Todos os modelos t√™m o mesmo peso              | Modelos com menor erro recebem maior peso        |\n",
        "| Peso nos exemplos        | N√£o altera                                     | Aumenta peso dos erros para focar nos dif√≠ceis   |\n",
        "| Toler√¢ncia a ru√≠do       | Alta (robusto a outliers)                      | Baixa (sens√≠vel a outliers)                      |\n",
        "| Complexidade             | Mais simples de ajustar                        | Requer mais cuidado (learning rate, overfitting) |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6OmuxDPoohnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 4. Implementa√ß√£o em Python\n",
        "\n",
        "Vamos usar o conjunto de dados Iris como exemplo e aplicar o AdaBoost, sendo demonstrando passo a passo de forma manual e depois usando scikit-learn\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aroFvdA8g551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° 4.1. AdaBoost com Dataset Iris - Manual\n",
        "\n",
        "Vamos entender o funcionamento do AdaBoost recriando suas 6 etapas principais com o dataset Iris.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6I6dXt7QWJBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 1: - Inicializa√ß√£o dos pesos\n",
        "\n",
        "Todos os exemplos do dataset come√ßam com o mesmo peso.\n",
        "Neste exemplo did√°tico, usaremos apenas duas classes e duas features para simplificar.\n"
      ],
      "metadata": {
        "id": "qYu1qlSGWYP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o dataset com duas classes e duas features\n",
        "iris = load_iris()\n",
        "X_full = iris.data[iris.target != 2][:, :2]  # Apenas classes 0 e 1, duas features\n",
        "y_full = iris.target[iris.target != 2]\n",
        "\n",
        "# Separar treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, random_state=42)\n",
        "\n",
        "# Inicializar pesos uniformemente (apenas no treino)\n",
        "n_amostras = len(y_train)\n",
        "pesos = np.ones(n_amostras) / n_amostras"
      ],
      "metadata": {
        "id": "ynLLi7V8XGe-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 2, 3 e 4 - Treinamento sequencial de modelos fracos ajustes dos pesos e c√°lculo dos pr√≥ximos pesos\n",
        "\n",
        "Treinamos um modelo fraco (stump) e calculamos o erro ponderado."
      ],
      "metadata": {
        "id": "wsYgVO5RXNXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "n_estimators = 5\n",
        "modelos = []\n",
        "alfas = []\n",
        "\n",
        "for i in range(n_estimators):\n",
        "    modelo = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "    modelo.fit(X_train, y_train, sample_weight=pesos)\n",
        "    pred = modelo.predict(X_train)\n",
        "\n",
        "    # Etapa 3 e 4: erro e peso do modelo\n",
        "    erro = np.sum(pesos * (pred != y_train)) / np.sum(pesos)\n",
        "    erro = np.clip(erro, 1e-10, 1 - 1e-10)\n",
        "    alfa = 0.5 * np.log((1 - erro) / erro)\n",
        "\n",
        "    # Etapa 3 - Ajuste dos pesos dos exemplos\n",
        "    pesos = pesos * np.exp(-alfa * y_train * (2 * (pred == y_train) - 1))\n",
        "    pesos = pesos / np.sum(pesos)\n",
        "\n",
        "    modelos.append(modelo)\n",
        "    alfas.append(alfa)\n",
        "\n",
        "    print(f\"üîÅ Itera√ß√£o {i+1} | Erro: {erro:.4f} | Alfa: {alfa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZE-euO9XWfg",
        "outputId": "34b8b79a-bbaf-4822-8403-2ad117d9be97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Itera√ß√£o 1 | Erro: 0.1143 | Alfa: 1.0238\n",
            "üîÅ Itera√ß√£o 2 | Erro: 0.1400 | Alfa: 0.9078\n",
            "üîÅ Itera√ß√£o 3 | Erro: 0.1685 | Alfa: 0.7982\n",
            "üîÅ Itera√ß√£o 4 | Erro: 0.1412 | Alfa: 0.9027\n",
            "üîÅ Itera√ß√£o 5 | Erro: 0.1797 | Alfa: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 5 - Repeti√ß√£o\n",
        "\n",
        "Repetimos esse processo para `n_estimators` vezes. No c√≥digo acima, usamos um loop com 5 itera√ß√µes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9mCOBIJ1Xxy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 6 - Agrega√ß√£o das previs√µes\n",
        "\n",
        "A predi√ß√£o final √© feita com uma vota√ß√£o ponderada."
      ],
      "metadata": {
        "id": "Yacuqy6RX9ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Vota√ß√£o ponderada no conjunto de teste\n",
        "predicoes = np.zeros_like(y_test, dtype=float)\n",
        "for modelo, alfa in zip(modelos, alfas):\n",
        "    pred = modelo.predict(X_test)\n",
        "    pred = np.where(pred == 1, 1, -1)\n",
        "    predicoes += alfa * pred\n",
        "\n",
        "final = np.where(predicoes > 0, 1, 0)\n",
        "print(\"üéØ Acur√°cia final do AdaBoost manual (com dados de teste):\", accuracy_score(y_test, final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4abY7pxYCES",
        "outputId": "d25e2512-0cc7-47ba-a2c4-707ce12f0054"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Acur√°cia final do AdaBoost manual (com dados de teste): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° 4.2. AdaBoost com Scikit-learn (autom√°tico)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlu-mwBNj90I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separar treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Criar AdaBoost automatizado\n",
        "modelo_base = DecisionTreeClassifier(max_depth=1)\n",
        "adaboost = AdaBoostClassifier(\n",
        "    estimator=modelo_base,\n",
        "    n_estimators=5,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinar e avaliar\n",
        "adaboost.fit(X_train, y_train)\n",
        "y_pred = adaboost.predict(X_test)\n",
        "print(\"ü§ñ Acur√°cia com AdaBoost (sklearn):\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsdlCeFkT62",
        "outputId": "473d1b19-abda-4c87-f486-8cc8e8ccd423"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Acur√°cia com AdaBoost (sklearn): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° Cinco hiperpar√¢metros importantes no AdaBoost\n",
        "\n",
        "Abaixo est√£o os hiperpar√¢metros mais relevantes no `AdaBoostClassifier` do scikit-learn:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5oVk7bJ2skE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Hiperpar√¢metro     | Descri√ß√£o                                                                                 |\n",
        "|--------------------|--------------------------------------------------------------------------------------------|\n",
        "| `n_estimators`     | N√∫mero de modelos fracos (√°rvores) a serem treinados sequencialmente.                     |\n",
        "| `learning_rate`    | Controla o peso dado a cada modelo. Valores menores tornam o aprendizado mais conservador.|\n",
        "| `estimator`        | Define o modelo fraco usado (ex: `DecisionTreeClassifier` com `max_depth=1`).             |\n",
        "| `algorithm`        | Escolhe o tipo de AdaBoost: `'SAMME'` (discreto) ou `'SAMME.R'` (real/probabil√≠stico).    |\n",
        "| `random_state`     | Semente aleat√≥ria para garantir reprodutibilidade.                                        |\n",
        "\n",
        "---\n",
        "\n",
        " üß† Observa√ß√µes adicionais:\n",
        "\n",
        "- **`n_estimators`**: quanto maior, mais modelos s√£o combinados. Por√©m, ap√≥s certo ponto, o ganho de performance pode estabilizar.\n",
        "- **`learning_rate`**: valores muito altos podem causar overfitting; muito baixos, underfitting. O valor padr√£o √© `1.0`.\n",
        "- **`estimator`**: normalmente usamos √°rvores rasas (stumps), mas voc√™ pode experimentar modelos mais complexos.\n",
        "- **`algorithm='SAMME.R'`**: recomendado para classifica√ß√£o multiclasse; usa probabilidades e costuma performar melhor.\n",
        "- **`random_state`**: garante que os resultados sejam reproduz√≠veis entre execu√ß√µes.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NfE_3Tehub6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Exemplo com GridSearchCV para ajuste de hiperpar√¢metros"
      ],
      "metadata": {
        "id": "-SVj5P0auzXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [5, 10, 50],\n",
        "    'learning_rate': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"üîç Melhores par√¢metros:\", grid.best_params_)\n",
        "print(\"‚úÖ Acur√°cia com GridSearchCV:\", accuracy_score(y_test, grid.best_estimator_.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB5AXvaWam7v",
        "outputId": "ec0345e6-ec5d-4c30-8336-14abdbf9e610"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "üîç Melhores par√¢metros: {'learning_rate': 1.0, 'n_estimators': 10}\n",
            "‚úÖ Acur√°cia com GridSearchCV: 1.0\n"
          ]
        }
      ]
    }
  ]
}