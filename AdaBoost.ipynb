{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMydrN6OXui2PnnCbPn1ZEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbonelli/Cientista-de-Dados_EBAC/blob/main/AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡  AdaBoost"
      ],
      "metadata": {
        "id": "-w76JuWKgUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ 1. O que é o AdaBoost?\n",
        "\n",
        "**AdaBoost** (Adaptive Boosting) é um algoritmo de aprendizado de máquina que combina vários modelos fracos (geralmente árvores simples, chamadas \"stumps\") para formar um modelo forte e mais preciso.\n",
        "\n",
        "A ideia central é **focar nos exemplos que o modelo anterior errou**, ajustando os pesos dos dados a cada rodada de treinamento. Isso permite que o modelo aprenda progressivamente com seus próprios erros.\n",
        "\n",
        "Ele funciona bem com classificadores fracos e é muito usado por sua simplicidade e boa performance — desde que os dados não tenham muito **ruído ou outliers**, já que o AdaBoost é sensível a isso.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JRHLPTmkgijm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ 2. Passo a passo do algoritmo AdaBoost\n",
        "\n",
        "1. **Inicialização dos pesos**  \n",
        "   - Todos os exemplos do dataset começam com o mesmo peso.\n",
        "\n",
        "2. **Treinamento sequencial de modelos fracos**  \n",
        "   - Treina-se um modelo (ex: árvore simples).\n",
        "   - Calcula-se o erro ponderado (considerando os pesos dos exemplos).\n",
        "\n",
        "3. **Ajuste dos pesos dos exemplos**  \n",
        "   - Exemplos que o modelo errou recebem mais peso (para ganhar mais atenção no próximo modelo).\n",
        "\n",
        "4. **Cálculo do peso do modelo**  \n",
        "   - Modelos com menor erro ganham mais peso na combinação final.\n",
        "\n",
        "5. **Repetição**  \n",
        "   - Repete-se o processo para `n_estimators` vezes.\n",
        "\n",
        "6. **Agregação das previsões**  \n",
        "   - A previsão final é feita com base em uma votação ponderada dos modelos.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ldS7zCnXgswv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ 3. Cinco diferenças entre Random Forest e AdaBoost\n",
        "\n",
        "| Característica           | **Random Forest**                              | **AdaBoost**                                      |\n",
        "|--------------------------|-------------------------------------------------|--------------------------------------------------|\n",
        "| Estratégia de ensemble   | Paralela (várias árvores ao mesmo tempo)       | Sequencial (um modelo por vez)                   |\n",
        "| Peso dos modelos         | Todos os modelos têm o mesmo peso              | Modelos com menor erro recebem maior peso        |\n",
        "| Peso nos exemplos        | Não altera                                     | Aumenta peso dos erros para focar nos difíceis   |\n",
        "| Tolerância a ruído       | Alta (robusto a outliers)                      | Baixa (sensível a outliers)                      |\n",
        "| Complexidade             | Mais simples de ajustar                        | Requer mais cuidado (learning rate, overfitting) |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6OmuxDPoohnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ 4. Implementação em Python\n",
        "\n",
        "Vamos usar o conjunto de dados Iris como exemplo e aplicar o AdaBoost, sendo demonstrando passo a passo de forma manual e depois usando scikit-learn\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aroFvdA8g551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ 4.1. AdaBoost com Dataset Iris - Manual\n",
        "\n",
        "Vamos entender o funcionamento do AdaBoost recriando suas 6 etapas principais com o dataset Iris.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6I6dXt7QWJBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Etapa 1: - Inicialização dos pesos\n",
        "\n",
        "Todos os exemplos do dataset começam com o mesmo peso.\n",
        "Neste exemplo didático, usaremos apenas duas classes e duas features para simplificar.\n"
      ],
      "metadata": {
        "id": "qYu1qlSGWYP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o dataset com duas classes e duas features\n",
        "iris = load_iris()\n",
        "X_full = iris.data[iris.target != 2][:, :2]  # Apenas classes 0 e 1, duas features\n",
        "y_full = iris.target[iris.target != 2]\n",
        "\n",
        "# Separar treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, random_state=42)\n",
        "\n",
        "# Inicializar pesos uniformemente (apenas no treino)\n",
        "n_amostras = len(y_train)\n",
        "pesos = np.ones(n_amostras) / n_amostras"
      ],
      "metadata": {
        "id": "ynLLi7V8XGe-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Etapa 2, 3 e 4 - Treinamento sequencial de modelos fracos ajustes dos pesos e cálculo dos próximos pesos\n",
        "\n",
        "Treinamos um modelo fraco (stump) e calculamos o erro ponderado."
      ],
      "metadata": {
        "id": "wsYgVO5RXNXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "n_estimators = 5\n",
        "modelos = []\n",
        "alfas = []\n",
        "\n",
        "for i in range(n_estimators):\n",
        "    modelo = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "    modelo.fit(X_train, y_train, sample_weight=pesos)\n",
        "    pred = modelo.predict(X_train)\n",
        "\n",
        "    # Etapa 3 e 4: erro e peso do modelo\n",
        "    erro = np.sum(pesos * (pred != y_train)) / np.sum(pesos)\n",
        "    erro = np.clip(erro, 1e-10, 1 - 1e-10)\n",
        "    alfa = 0.5 * np.log((1 - erro) / erro)\n",
        "\n",
        "    # Etapa 3 - Ajuste dos pesos dos exemplos\n",
        "    pesos = pesos * np.exp(-alfa * y_train * (2 * (pred == y_train) - 1))\n",
        "    pesos = pesos / np.sum(pesos)\n",
        "\n",
        "    modelos.append(modelo)\n",
        "    alfas.append(alfa)\n",
        "\n",
        "    print(f\"🔁 Iteração {i+1} | Erro: {erro:.4f} | Alfa: {alfa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZE-euO9XWfg",
        "outputId": "34b8b79a-bbaf-4822-8403-2ad117d9be97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Iteração 1 | Erro: 0.1143 | Alfa: 1.0238\n",
            "🔁 Iteração 2 | Erro: 0.1400 | Alfa: 0.9078\n",
            "🔁 Iteração 3 | Erro: 0.1685 | Alfa: 0.7982\n",
            "🔁 Iteração 4 | Erro: 0.1412 | Alfa: 0.9027\n",
            "🔁 Iteração 5 | Erro: 0.1797 | Alfa: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Etapa 5 - Repetição\n",
        "\n",
        "Repetimos esse processo para `n_estimators` vezes. No código acima, usamos um loop com 5 iterações.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9mCOBIJ1Xxy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 Etapa 6 - Agregação das previsões\n",
        "\n",
        "A predição final é feita com uma votação ponderada."
      ],
      "metadata": {
        "id": "Yacuqy6RX9ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Votação ponderada no conjunto de teste\n",
        "predicoes = np.zeros_like(y_test, dtype=float)\n",
        "for modelo, alfa in zip(modelos, alfas):\n",
        "    pred = modelo.predict(X_test)\n",
        "    pred = np.where(pred == 1, 1, -1)\n",
        "    predicoes += alfa * pred\n",
        "\n",
        "final = np.where(predicoes > 0, 1, 0)\n",
        "print(\"🎯 Acurácia final do AdaBoost manual (com dados de teste):\", accuracy_score(y_test, final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4abY7pxYCES",
        "outputId": "d25e2512-0cc7-47ba-a2c4-707ce12f0054"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Acurácia final do AdaBoost manual (com dados de teste): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ 4.2. AdaBoost com Scikit-learn (automático)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlu-mwBNj90I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separar treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Criar AdaBoost automatizado\n",
        "modelo_base = DecisionTreeClassifier(max_depth=1)\n",
        "adaboost = AdaBoostClassifier(\n",
        "    estimator=modelo_base,\n",
        "    n_estimators=5,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinar e avaliar\n",
        "adaboost.fit(X_train, y_train)\n",
        "y_pred = adaboost.predict(X_test)\n",
        "print(\"🤖 Acurácia com AdaBoost (sklearn):\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsdlCeFkT62",
        "outputId": "473d1b19-abda-4c87-f486-8cc8e8ccd423"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Acurácia com AdaBoost (sklearn): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Cinco hiperparâmetros importantes no AdaBoost\n",
        "\n",
        "Abaixo estão os hiperparâmetros mais relevantes no `AdaBoostClassifier` do scikit-learn:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5oVk7bJ2skE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Hiperparâmetro     | Descrição                                                                                 |\n",
        "|--------------------|--------------------------------------------------------------------------------------------|\n",
        "| `n_estimators`     | Número de modelos fracos (árvores) a serem treinados sequencialmente.                     |\n",
        "| `learning_rate`    | Controla o peso dado a cada modelo. Valores menores tornam o aprendizado mais conservador.|\n",
        "| `estimator`        | Define o modelo fraco usado (ex: `DecisionTreeClassifier` com `max_depth=1`).             |\n",
        "| `algorithm`        | Escolhe o tipo de AdaBoost: `'SAMME'` (discreto) ou `'SAMME.R'` (real/probabilístico).    |\n",
        "| `random_state`     | Semente aleatória para garantir reprodutibilidade.                                        |\n",
        "\n",
        "---\n",
        "\n",
        " 🧠 Observações adicionais:\n",
        "\n",
        "- **`n_estimators`**: quanto maior, mais modelos são combinados. Porém, após certo ponto, o ganho de performance pode estabilizar.\n",
        "- **`learning_rate`**: valores muito altos podem causar overfitting; muito baixos, underfitting. O valor padrão é `1.0`.\n",
        "- **`estimator`**: normalmente usamos árvores rasas (stumps), mas você pode experimentar modelos mais complexos.\n",
        "- **`algorithm='SAMME.R'`**: recomendado para classificação multiclasse; usa probabilidades e costuma performar melhor.\n",
        "- **`random_state`**: garante que os resultados sejam reproduzíveis entre execuções.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NfE_3Tehub6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ Exemplo com GridSearchCV para ajuste de hiperparâmetros"
      ],
      "metadata": {
        "id": "-SVj5P0auzXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [5, 10, 50],\n",
        "    'learning_rate': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"🔍 Melhores parâmetros:\", grid.best_params_)\n",
        "print(\"✅ Acurácia com GridSearchCV:\", accuracy_score(y_test, grid.best_estimator_.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB5AXvaWam7v",
        "outputId": "ec0345e6-ec5d-4c30-8336-14abdbf9e610"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "🔍 Melhores parâmetros: {'learning_rate': 1.0, 'n_estimators': 10}\n",
            "✅ Acurácia com GridSearchCV: 1.0\n"
          ]
        }
      ]
    }
  ]
}