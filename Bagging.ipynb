{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOh1IXLUNGUTHbtdwpPv55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbonelli/Cientista-de-Dados_EBAC/blob/main/Bagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Bagging"
      ],
      "metadata": {
        "id": "-w76JuWKgUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. O que √© Bagging\n",
        "\n",
        "O **Bagging** (de *Bootstrap Aggregating*) √© uma t√©cnica usada para melhorar o desempenho de modelos de Machine Learning. A ideia √© simples:\n",
        "\n",
        "- Criar v√°rias amostras diferentes dos dados (com reposi√ß√£o);\n",
        "- Treinar modelos separados em cada uma dessas amostras;\n",
        "- Combinar as previs√µes desses modelos para tomar uma decis√£o mais robusta.\n",
        "\n",
        "Isso reduz o **overfitting** e melhora a **generaliza√ß√£o**. Um exemplo famoso que usa Bagging √© o **Random Forest**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JRHLPTmkgijm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úÖ 2. Passo a passo para o Bagging\n",
        "\n",
        "1. **Bootstrap (amostragem com reposi√ß√£o)**  \n",
        "   - Gerar v√°rias amostras do conjunto de dados original com reposi√ß√£o.  \n",
        "   - Cada amostra ter√° o mesmo tamanho do dataset original, mas com dados repetidos ou omitidos aleatoriamente.\n",
        "\n",
        "2. **Modelagem (treinamento de modelos)**  \n",
        "   - Treinar um modelo (como uma √°rvore de decis√£o) em cada uma dessas amostras bootstrap.  \n",
        "   - Cada modelo vai aprender de forma levemente diferente por causa da varia√ß√£o nas amostras.\n",
        "\n",
        "3. **Agrega√ß√£o (combinar os modelos)**  \n",
        "   - Para **classifica√ß√£o**: usar vota√ß√£o majorit√°ria (a classe mais votada vence).  \n",
        "   - Para **regress√£o**: calcular a m√©dia das previs√µes dos modelos.\n",
        "\n",
        "   ---"
      ],
      "metadata": {
        "id": "ldS7zCnXgswv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. Implementa√ß√£o em Python\n",
        "\n",
        "Vamos usar o conjunto de dados Iris como exemplo e aplicar o Bagging com √°rvores de decis√£o, sendo demonstrando passo a passo de forma manual e depois usando scikit-learn\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aroFvdA8g551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üå∏ 3.1. Bagging com o Dataset Iris - Manual\n",
        "\n",
        "Vamos entender o funcionamento do Bagging recriando suas 3 etapas principais com o dataset Iris.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9ZeGybOtio5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Etapa 1: Bootstrap (amostragem com reposi√ß√£o)"
      ],
      "metadata": {
        "id": "Cpz7qHvhkxAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Carregar dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Criar DataFrame para bootstrap\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Criar amostras com reposi√ß√£o (bootstrap)\n",
        "n = len(df)\n",
        "amostra1 = df.sample(n=n, replace=True, random_state=1).reset_index(drop=True)\n",
        "amostra2 = df.sample(n=n, replace=True, random_state=2).reset_index(drop=True)\n",
        "amostra3 = df.sample(n=n, replace=True, random_state=3).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "k339usHWi5LN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Etapa 2: Modelagem com √Årvores de Decis√£o\n"
      ],
      "metadata": {
        "id": "JUKjM5Mwi8wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir dados originais (n√£o os de bootstrap) para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Treinar os 3 modelos nas amostras bootstrap\n",
        "modelo1 = DecisionTreeClassifier().fit(amostra1[feature_names], amostra1['target'])\n",
        "modelo2 = DecisionTreeClassifier().fit(amostra2[feature_names], amostra2['target'])\n",
        "modelo3 = DecisionTreeClassifier().fit(amostra3[feature_names], amostra3['target'])"
      ],
      "metadata": {
        "id": "QaqpqeD_i_Kl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Etapa 3: Agrega√ß√£o (Vota√ß√£o entre os modelos)\n"
      ],
      "metadata": {
        "id": "nvibPlZGjNY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Converter X_test em DataFrame com nomes de colunas\n",
        "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "\n",
        "# Prever com os 3 modelos\n",
        "pred1 = modelo1.predict(X_test_df)\n",
        "pred2 = modelo2.predict(X_test_df)\n",
        "pred3 = modelo3.predict(X_test_df)\n",
        "\n",
        "# Vota√ß√£o: empilha e tira a moda linha a linha\n",
        "votos = np.array([pred1, pred2, pred3])\n",
        "votacao_final = mode(votos, axis=0, keepdims=True).mode[0]\n",
        "\n",
        "# Avalia√ß√£o\n",
        "print(\"‚úÖ Acur√°cia do Bagging manual:\", accuracy_score(y_test, votacao_final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6-MVLYwjPAB",
        "outputId": "bb6716c8-a833-4866-a3aa-5cf69be1f84f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Acur√°cia do Bagging manual: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° 3.2. Bagging com Scikit-learn (autom√°tico)\n",
        "\n",
        "Agora vamos usar o `BaggingClassifier`, que j√° faz tudo isso internamente de forma mais r√°pida e eficiente.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlu-mwBNj90I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ  Etapa 1: Carregando o Dataset Iris"
      ],
      "metadata": {
        "id": "RmMrrFHzkZKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "uQDaSRSqhJyE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≤  Etapa 2: Criando o Bagging com √Årvores de Decis√£o\n"
      ],
      "metadata": {
        "id": "JQEt9UN4kJB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Modelo base\n",
        "base = DecisionTreeClassifier()\n",
        "\n",
        "# Bagging com 10 √°rvores\n",
        "bagging = BaggingClassifier(estimator=base, n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Previs√µes e avalia√ß√£o\n",
        "y_pred = bagging.predict(X_test)\n",
        "print(\"‚úÖ Acur√°cia com BaggingClassifier:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsdlCeFkT62",
        "outputId": "368a2923-bfee-46f7-b595-75dc1c9a6f88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Acur√°cia com BaggingClassifier: 1.0\n"
          ]
        }
      ]
    }
  ]
}